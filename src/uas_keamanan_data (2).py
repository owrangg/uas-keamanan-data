# -*- coding: utf-8 -*-
"""UAS Keamanan Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hYCMJ6LU_iQbpb4DhS2z8u-OJjJ29_TZ
"""

import pandas as pd
import numpy as np
import re
import string
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

"""### Import Dataset"""

df = pd.read_csv('../data/spam.csv', encoding='latin-1')

df.head()

df.info()

"""### Preprocessing"""

df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])

df = df.rename(columns={'v1': 'label', 'v2': 'message'})

df.head()

df.info()

df['length'] = df['message'].apply(len)

df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})

"""#### Uppercase Count"""

def count_uppercase_ratio(text):
    total_chars = len(text)
    if total_chars == 0:
        return 0
    uppercase_count = sum(c.isupper() for c in text)
    return uppercase_count / total_chars

df['uppercase_ratio'] = df['message'].apply(count_uppercase_ratio)
df.head()

df.head()

"""### Jumlah sample"""

df.shape

"""### Distribusi kelas (label)"""

class_distribution = df['label'].value_counts()
class_distribution

# Hitung persentase
total_samples = df.shape[0]
percentage_ham = (class_distribution['ham'] / total_samples) * 100
percentage_spam = (class_distribution['spam'] / total_samples) * 100

print(f"Persentase pesan Ham (Bukan Spam): {percentage_ham:.2f}%")
print(f"Persentase pesan Spam: {percentage_spam:.2f}%")

"""### Pengecekan nilai missing value"""

missing_values = df[['label', 'message', 'length']].isnull().sum()
missing_values

"""### EDA

#### Distribusi Kelas (Class Imbalance)
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Menghitung jumlah masing-masing kelas
label_counts = df['label'].value_counts()
total = len(df) # Total seluruh baris data

# Menghitung persentase untuk setiap kelas
persentase = (label_counts / total) * 100

# Menampilkan hasil di console
print("Distribusi Label:")
print(label_counts)
print("\nPersentase Label:")
print(persentase)

# Membuat visualisasi bar chart
plt.figure(figsize=(10, 8))

# Menyimpan objek barplot ke variabel 'bars' agar bisa diiterasi
bars = sns.barplot(x=label_counts.index, y=label_counts.values, palette=['skyblue', 'pink'])

plt.title('Distribusi Kelas "ham" vs "spam"')
plt.xlabel('Label Pesan')
plt.ylabel('Jumlah Pesan')

# Menambahkan label nilai (Count dan Persentase) di atas bar
# Menggunakan zip untuk mengiterasi Patches (objek bar), Count, dan Persentase secara bersamaan
for bar, count, perc in zip(bars.patches, label_counts.values, persentase.values):
    # Format teks: Count (Persentase dengan satu desimal)
    text = f'{count} ({perc:.1f}%)'

    # Menentukan posisi teks
    height = bar.get_height()
    # Teks diletakkan di tengah horizontal, dan sedikit di atas bar vertikal
    plt.text(bar.get_x() + bar.get_width() / 2.,
             height + 50, # Nilai 50 adalah offset agar teks tidak menempel pada bar
             text,
             ha='center', # Horizontal Alignment: Center
             va='bottom', # Vertical Alignment: Bottom
             fontsize=12)

plt.show()

"""#### Analisis Panjang Pesan (Message Length)"""

#  Statistik deskriptif panjang pesan berdasarkan label
print("\nStatistik Deskriptif Panjang Pesan:")
print(df.groupby('label')['length'].describe())

# Visualisasi Distribusi Panjang Pesan (Histogram)
plt.figure(figsize=(12, 5))
df[df['label'] == 'ham']['length'].hist(bins=50, alpha=0.7, label='Ham', color='skyblue')
df[df['label'] == 'spam']['length'].hist(bins=50, alpha=0.7, label='Spam', color='salmon')
plt.legend()
plt.title('Perbandingan Distribusi Panjang Pesan (Ham vs. Spam)')
plt.xlabel('Panjang Pesan (Karakter)')
plt.ylabel('Frekuensi')
plt.show()

"""#### Word Cloud (Visualisasi Kata Kunci)"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt # Ensure plt is imported for plotting
import seaborn as sns # Ensure sns is imported if needed for other visualizations

# Pisahkan pesan Spam dan Ham
spam_words = ' '.join(list(df[df['label'] == 'spam']['message']))
ham_words = ' '.join(list(df[df['label'] == 'ham']['message']))

# Word Cloud untuk Spam
spam_wc = WordCloud(width=800, height=500, background_color='white', max_words=100, colormap='Reds').generate(spam_words)
plt.figure(figsize=(10, 5))
plt.imshow(spam_wc, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud: Pesan SPAM')
plt.show()

# Word Cloud untuk Ham
ham_wc = WordCloud(width=800, height=500, background_color='white', max_words=100, colormap='Blues').generate(ham_words)
plt.figure(figsize=(10, 5))
plt.imshow(ham_wc, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud: Pesan HAM')
plt.show()

spam_freq_dict_spam = spam_wc.words_
spam_df = pd.DataFrame(list(spam_freq_dict_spam.items()), columns=['Kata', 'Frekuensi_Relatif'])

top_spam = spam_df.nlargest(20, 'Frekuensi_Relatif')

total_freq_relatif_spam = top_spam['Frekuensi_Relatif'].sum()

top_spam['Persentase'] = (top_spam['Frekuensi_Relatif'] / total_freq_relatif_spam) * 100

plt.figure(figsize=(12, 8))
bars = sns.barplot(x='Persentase', y='Kata', data=top_spam, palette='Reds_r') # palette Reds untuk SPAM

plt.title('Pesan SPAM (Berdasarkan Kontribusi Frekuensi)')
plt.xlabel('Persentase Kontribusi Frekuensi')
plt.ylabel('Kata')

for bar in bars.patches:
    percentage = bar.get_width()
    text = f'{percentage:.1f}%'

    x = bar.get_width()
    y = bar.get_y() + bar.get_height() / 2
    plt.text(x + 0.5, y, text, va='center', fontsize=10)

plt.show()

ham_wc = WordCloud(width=800, height=500, background_color='white', max_words=100, colormap='Blues').generate(ham_words)

spam_freq_dict_ham = ham_wc.words_
spam_df = pd.DataFrame(list(spam_freq_dict_ham.items()), columns=['Kata', 'Frekuensi_Relatif'])

top_spam = spam_df.nlargest(20, 'Frekuensi_Relatif')

total_freq_relatif_spam = top_spam['Frekuensi_Relatif'].sum()


top_spam['Persentase'] = (top_spam['Frekuensi_Relatif'] / total_freq_relatif_spam) * 100

plt.figure(figsize=(12, 8))
bars = sns.barplot(x='Persentase', y='Kata', data=top_spam, palette='Blues_r') # palette Blues untuk HAM

plt.title('Pesan HAM (Berdasarkan Kontribusi Frekuensi)')
plt.xlabel('Persentase Kontribusi Frekuensi')
plt.ylabel('Kata')

for bar in bars.patches:
    percentage = bar.get_width()
    text = f'{percentage:.1f}%'

    x = bar.get_width()
    y = bar.get_y() + bar.get_height() / 2
    plt.text(x + 0.5, y, text, va='center', fontsize=10)

plt.show()

"""#### Uppercase Count"""

plt.figure(figsize=(10, 6))

# Box Plot untuk melihat median dan distribusi
sns.boxplot(x='label', y='uppercase_ratio', data=df, palette=['skyblue', 'salmon'])
plt.title('Proporsi Huruf Kapital dalam Pesan (Ham vs Spam)')
plt.xlabel('Label Pesan')
plt.ylabel('Rasio Huruf Kapital / Total Karakter')
plt.show()

"""### Text preprocessing

#### Case folding
"""

df['lower_case'] = df['message'].str.lower()

spam_index = 2
ham_index = 0

# Perbandingan Pesan Spam
print(f"Pesan Spam (Asli):     '{df['message'].iloc[spam_index]}'")
print(f"Pesan Spam (Lowercase):'{df['lower_case'].iloc[spam_index]}'")

print("\n-----------------------------------------------------")

# Perbandingan Pesan Ham
print(f"Pesan Ham (Asli):      '{df['message'].iloc[ham_index]}'")
print(f"Pesan Ham (Lowercase): '{df['lower_case'].iloc[ham_index]}'")
print("-----------------------------------------------------")

"""#### Penghapusan Tanda Baca, Karakter Khusus, dan Angka"""

def remove_punctuation_numbers(text):
    # Hapus angka
    text = re.sub(r'\d+', '', text)
    # Hapus tanda baca
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

df['remove_punctuation'] = df['lower_case'].apply(remove_punctuation_numbers)

spam_index = 2
ham_index = 0

# Perbandingan Pesan Spam
print("--- Contoh Pesan SPAM ---")
print(f"Sebelum Pembersihan: '{df['lower_case'].iloc[spam_index]}'")
print(f"Setelah Pembersihan: '{df['remove_punctuation'].iloc[spam_index]}'")

print("\n----------------------------------------------------------")

# Perbandingan Pesan Ham
print("--- Contoh Pesan HAM ---")
print(f"Sebelum Pembersihan: '{df['lower_case'].iloc[ham_index]}'")
print(f"Setelah Pembersihan: '{df['remove_punctuation'].iloc[ham_index]}'")
print("----------------------------------------------------------")

"""#### Stop word removal"""

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    # Tokenization implisit: memecah string menjadi kata-kata (split)
    words = text.split()
    # Hapus stop words
    words = [word for word in words if word not in stop_words]
    # Gabungkan kembali
    return ' '.join(words)

# Menerapkan penghapusan stop words
df['stopwords'] = df['remove_punctuation'].apply(remove_stopwords)

# --- Perbandingan Hasil ---
spam_index = 2
ham_index = 0

# Perbandingan Pesan Spam
print("--- Contoh Pesan SPAM ---")
print(f"Sebelum Stop Word Removal: '{df['remove_punctuation'].iloc[spam_index]}'")
print(f"Setelah Stop Word Removal: '{df['stopwords'].iloc[spam_index]}'")

print("\n----------------------------------------------------------")

# Perbandingan Pesan Ham
print("--- Contoh Pesan HAM ---")
print(f"Sebelum Stop Word Removal: '{df['remove_punctuation'].iloc[ham_index]}'")
print(f"Setelah Stop Word Removal: '{df['stopwords'].iloc[ham_index]}'")
print("----------------------------------------------------------")

"""#### Lemmatizaion"""

lemmatizer = WordNetLemmatizer()

def apply_lemmatization(text):
    # Tokenization implisit
    words = text.split()
    # Lakukan Lemmatization
    words = [lemmatizer.lemmatize(word) for word in words]
    # Gabungkan kembali
    return ' '.join(words)

# Apply lemmatization to create the new column
df['lemmatization'] = df['stopwords'].apply(apply_lemmatization)

# --- Perbandingan Hasil ---
spam_index = 2
ham_index = 0

# Perbandingan Pesan Spam
print("--- Contoh Pesan SPAM ---")
print(f"Sebelum Lemmatization: '{df['stopwords'].iloc[spam_index]}'")
print(f"Setelah Lemmatization: '{df['lemmatization'].iloc[spam_index]}'")

print("\n----------------------------------------------------------")

# Perbandingan Pesan Ham
print("--- Contoh Pesan HAM ---")
print(f"Sebelum Lemmatization: '{df['stopwords'].iloc[ham_index]}'")
print(f"Setelah Lemmatization: '{df['lemmatization'].iloc[ham_index]}'")
print("----------------------------------------------------------")

"""### Feature Consolidation & Scaling"""

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf_sparse = tfidf_vectorizer.fit_transform(df['lemmatization'])
X_tfidf = pd.DataFrame(X_tfidf_sparse.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

scaler = StandardScaler()
X_length_scaled = scaler.fit_transform(df[['length']])
X_length_scaled_df = pd.DataFrame(X_length_scaled, columns=['length_scaled'])

X_final = pd.concat([X_tfidf, X_length_scaled_df], axis=1)

print("\n--- Konsolidasi dan Scaling Fitur ---")
print(f"Bentuk Fitur Akhir (Termasuk length_scaled): {X_final.shape}")
print("Statistik Deskriptif Fitur 'length_scaled':")
print(X_final['length_scaled'].describe())

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf_sparse = tfidf_vectorizer.fit_transform(df['lemmatization'])
X_tfidf = pd.DataFrame(X_tfidf_sparse.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

scaler = StandardScaler()
X_length_scaled = scaler.fit_transform(df[['length']])
X_length_scaled_df = pd.DataFrame(X_length_scaled, columns=['length_scaled'])

X_final = pd.concat([X_tfidf, X_length_scaled_df], axis=1)
Y = df['label_num']

"""### Splitting dataset"""

X_train, X_temp, Y_train, Y_temp = train_test_split(
    X_final, Y,
    test_size=0.3,
    random_state=42,
    stratify=Y
)

# test_size=0.5 karena kita membagi 30% menjadi dua bagian (15% dan 15%)
X_val, X_test, Y_val, Y_test = train_test_split(
    X_temp, Y_temp,
    test_size=0.5,
    random_state=42,
    stratify=Y_temp
)

# --- Tampilkan Hasil Pembagian ---
print("\n--- Hasil Akhir Pembagian Data (70% Train / 15% Validation / 15% Test) ---")
print(f"Total Sampel: {X_final.shape[0]}")
print("--------------------------------------------------")
print(f"Training Set (70%):   {X_train.shape[0]} sampel (Fitur: {X_train.shape[1]})")
print(f"Validation Set (15%): {X_val.shape[0]} sampel (Fitur: {X_val.shape[1]})")
print(f"Test Set (15%):       {X_test.shape[0]} sampel (Fitur: {X_test.shape[1]})")
print("--------------------------------------------------")

"""## Bangun model deteksi

### Random forest
"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced',
    n_jobs=-1
)

print("Memulai pelatihan Random Forest Classifier...")
rf_model.fit(X_train, Y_train)

Y_pred_rf = rf_model.predict(X_val)

print("✅ Model Random Forest berhasil dilatih.")

from sklearn.metrics import classification_report, confusion_matrix

print("Confusion Matrix:")
print(confusion_matrix(Y_val, Y_pred_rf))

print("\n--------------------------------------------------------------------")

print("Classification Report:")
print(classification_report(Y_val, Y_pred_rf, target_names=['Ham (0)', 'Spam (1)']))

cm_rf = confusion_matrix(Y_val, Y_pred_rf)

TN, FP, FN, TP = cm_rf.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'],
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model Random Forest')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.show()

cm_rf = confusion_matrix(Y_val, Y_pred_rf)

class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_rf,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: Random Forest')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

TN, FP, FN, TP = cm_rf.ravel()
plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_proba_rf = rf_model.predict_proba(X_val)[:, 1]

fpr_rf, tpr_rf, thresholds_rf = roc_curve(Y_val, Y_pred_proba_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_rf,
    tpr_rf,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC Random Forest (AUC = {roc_auc_rf:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) Random Forest')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"AUC (Area Under the Curve) Random Forest: {roc_auc_rf:.4f}")

importances_rf = rf_model.feature_importances_

feature_names = X_train.columns

df_importance_rf = pd.DataFrame({
    'feature': feature_names,
    'importance': importances_rf
})

df_importance_rf = df_importance_rf.sort_values(by='importance', ascending=False).head(10)

print(df_importance_rf)

plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_rf,
    color='skyblue'
)
plt.title('Feature Importance (Random Forest)')
plt.xlabel('Importance Score (Gini Importance)')
plt.ylabel('Fitur (Kata Kunci/Panjang Pesan)')
plt.show()

"""### Logistic regression"""

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(
    solver='liblinear',
    random_state=42,
    class_weight='balanced'
)

print("Memulai pelatihan Logistic Regression...")
lr_model.fit(X_train, Y_train)

Y_pred_lr = lr_model.predict(X_val)

print("✅ Model Logistic Regression berhasil dilatih.")

print(confusion_matrix(Y_val, Y_pred_lr))

print("\n--------------------------------------------------------------------")

print("Classification Report:")
print(classification_report(Y_val, Y_pred_lr, target_names=['Ham (0)', 'Spam (1)']))

cm_lr = confusion_matrix(Y_val, Y_pred_lr)

TN, FP, FN, TP = cm_lr.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'],
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model Logistic Regression')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

cm_lr = confusion_matrix(Y_val, Y_pred_lr)

class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_lr,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: Logistic Regression')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

TN, FP, FN, TP = cm_lr.ravel()

plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_proba_lr = lr_model.predict_proba(X_val)[:, 1]

fpr_lr, tpr_lr, thresholds_lr = roc_curve(Y_val, Y_pred_proba_lr)

roc_auc_lr = auc(fpr_lr, tpr_lr)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_lr,
    tpr_lr,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC Regresi Logistik (AUC = {roc_auc_lr:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) Logistic Regression') # Mengganti Judul
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"AUC (Area Under the Curve) Logistic Regression: {roc_auc_lr:.4f}")

coefficients_lr = lr_model.coef_[0]

importance_abs_lr = np.abs(coefficients_lr)

feature_names = X_train.columns
df_importance_lr = pd.DataFrame({
    'feature': feature_names,
    'importance': importance_abs_lr,
    'coefficient': coefficients_lr
})

df_importance_lr = df_importance_lr.sort_values(
    by='importance',
    ascending=False
).head(10)

print("--- 10 Fitur Paling Penting (Berdasarkan Nilai Absolut Koefisien LR) ---")
print(df_importance_lr)


plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_lr,
    color='skyblue'
)
plt.title('Feature Importance (Logistic Regression)')
plt.xlabel('Nilai Absolut Koefisien (Tingkat Pengaruh)')
plt.ylabel('Fitur (Kata Kunci/Panjang Pesan)')
plt.show()

"""### Isolation forest"""

from sklearn.ensemble import IsolationForest

spam_ratio = Y.sum() / Y.shape[0]

if_model = IsolationForest(
    n_estimators=100,
    contamination=spam_ratio,
    random_state=42,
    n_jobs=-1
)

print("Memulai pelatihan Isolation Forest...")
if_model.fit(X_final)

df['anomaly_score'] = if_model.decision_function(X_final)
df['if_prediction'] = if_model.predict(X_final)

print("✅ Model Isolation Forest berhasil dilatih dan diprediksi.")

df['if_prediction_binary'] = df['if_prediction'].map({1: 0, -1: 1})

cm_if = confusion_matrix(df['label_num'], df['if_prediction_binary'])

print(f"Rasio Spam yang Diestimasikan (Contamination): {spam_ratio:.4f}")
print("--------------------------------------------------------------------")

print("Confusion Matrix (Asli vs. Prediksi IF):")
print(cm_if)

print("Classification Report:")
print(classification_report(df['label_num'], df['if_prediction_binary'], target_names=['Ham (0)', 'Spam (1)']))

TN, FP, FN, TP = cm_if.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'] ,
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model Isolation Forest')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print(f"False Positives (FP): {FP}, False Negatives (FN): {FN}")

class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_if,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: Isolation Forest (Anomaly Detection)')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_score_if = -df['anomaly_score']
Y_val_if = df['label_num']

fpr_if, tpr_if, thresholds_if = roc_curve(Y_val_if, Y_pred_score_if)

roc_auc_if = auc(fpr_if, tpr_if)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_if,
    tpr_if,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC Isolation Forest (AUC = {roc_auc_if:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) Isolation Forest')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"AUC (Area Under the Curve) Isolation Forest: {roc_auc_if:.4f}")

df_corr = X_val.copy()
df_corr['anomaly_score'] = df['anomaly_score']

correlation_with_score = df_corr.corr()['anomaly_score'].drop('anomaly_score')

importance_abs_corr = np.abs(correlation_with_score)

feature_names = importance_abs_corr.index
df_importance_if = pd.DataFrame({
    'feature': feature_names,
    'importance': importance_abs_corr
})

df_importance_if = df_importance_if.sort_values(
    by='importance',
    ascending=False
).head(10)

print("--- 10 Fitur Paling Penting (Korelasi Absolut dengan Skor Anomali IF) ---")
print(df_importance_if)

plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_if,
    color='skyblue'
)
plt.title('Feature Importance Isolation Forest')
plt.xlabel('Nilai Absolut Korelasi (Tingkat Pengaruh)')
plt.ylabel('Fitur')
plt.show()

"""### XGBoost"""

from xgboost import XGBClassifier

scale_pos_weight = Y_train.value_counts()[0] / Y_train.value_counts()[1]

xgb_model = XGBClassifier(
    objective='binary:logistic',
    n_estimators=100,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss',
    n_jobs=-1
)

print("Memulai pelatihan XGBoost Classifier...")
# XGBoost dapat dilatih langsung pada array numpy (yang merupakan X_train dan Y_train)
xgb_model.fit(X_train, Y_train)

# Prediksi pada Validation Set
Y_pred_xgb = xgb_model.predict(X_val)

print("✅ Model XGBoost berhasil dilatih.")

print("Confusion Matrix:")
print(confusion_matrix(Y_val, Y_pred_xgb))

print("\n--------------------------------------------------------------------")

print("Classification Report:")
print(classification_report(Y_val, Y_pred_xgb, target_names=['Ham (0)', 'Spam (1)']))

cm_xgb = confusion_matrix(Y_val, Y_pred_xgb)

TN, FP, FN, TP = cm_xgb.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'],
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model XGBoost')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

cm_xgb = confusion_matrix(Y_val, Y_pred_xgb)
TN, FP, FN, TP = cm_xgb.ravel()


class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_xgb,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: XGBoost Classifier')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]

fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(Y_val, Y_pred_proba_xgb)

roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_xgb,
    tpr_xgb,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC XGBoost (AUC = {roc_auc_xgb:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) XGBoost Classifier') # Mengganti Judul
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"AUC (Area Under the Curve) XGBoost: {roc_auc_xgb:.4f}")

importances_xgb = xgb_model.feature_importances_

feature_names = X_train.columns
df_importance_xgb = pd.DataFrame({
    'feature': feature_names,
    'importance': importances_xgb
})

df_importance_xgb = df_importance_xgb.sort_values(
    by='importance',
    ascending=False
).head(10)

print("--- XGBoost Feature Importance ---")
print(df_importance_xgb)


plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_xgb,
    color='skyblue'
)
plt.title('Feature Importance (XGBoost Classifier)')
plt.xlabel('Importance Score (Gain/Bobot)')
plt.ylabel('Fitur')
plt.show()

"""### LightGBM"""

from lightgbm import LGBMClassifier

scale_pos_weight_lgbm = Y_train.value_counts()[0] / Y_train.value_counts()[1]

lgbm_model = LGBMClassifier(
    objective='binary',
    n_estimators=100,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight_lgbm,
    random_state=42,
    n_jobs=-1
)

print("Memulai pelatihan LightGBM Classifier...")
lgbm_model.fit(X_train, Y_train)


Y_pred_lgbm = lgbm_model.predict(X_val)

print("✅ Model LightGBM berhasil dilatih.")

print("Confusion Matrix:")
print(confusion_matrix(Y_val, Y_pred_lgbm))

print("\n--------------------------------------------------------------------")

print("Classification Report:")
print(classification_report(Y_val, Y_pred_lgbm, target_names=['Ham (0)', 'Spam (1)']))

cm_lgbm = confusion_matrix(Y_val, Y_pred_lgbm)

TN, FP, FN, TP = cm_lgbm.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'],
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model LightGBM')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print(f"False Positives (FP): {FP}, False Negatives (FN): {FN}")

cm_lgbm = confusion_matrix(Y_val, Y_pred_lgbm)
TN, FP, FN, TP = cm_lgbm.ravel()


class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_lgbm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: LightGBM Classifier')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_proba_lgbm = lgbm_model.predict_proba(X_val)[:, 1]

fpr_lgbm, tpr_lgbm, thresholds_lgbm = roc_curve(Y_val, Y_pred_proba_lgbm)

roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_lgbm,
    tpr_lgbm,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC LightGBM (AUC = {roc_auc_lgbm:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) LightGBM Classifier') # Mengganti Judul
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Tampilkan nilai AUC
print(f"AUC (Area Under the Curve) LightGBM: {roc_auc_lgbm:.4f}")

importances_lgbm = lgbm_model.feature_importances_

feature_names = X_train.columns
df_importance_lgbm = pd.DataFrame({
    'feature': feature_names,
    'importance': importances_lgbm
})

df_importance_lgbm = df_importance_lgbm.sort_values(
    by='importance',
    ascending=False
).head(10)

print("--- LightGBM Feature Importance ---")

plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_lgbm,
    color='skyblue'
)
plt.title('Feature Importance (LightGBM Classifier)')
plt.xlabel('Importance Score (Gain/Frekuensi)')
plt.ylabel('Fitur')
plt.show()

"""### CNN"""

MAX_WORDS = 10000
EMBEDDING_DIM = 128
MAX_LEN = 200

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(df['lemmatization'])

sequences = tokenizer.texts_to_sequences(df['lemmatization'])

padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')

X_train_pad = padded_sequences[X_train.index]
X_val_pad = padded_sequences[X_val.index]
X_test_pad = padded_sequences[X_test.index]

cnn_model = Sequential([
    Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(10, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

cnn_model.compile(optimizer='adam',
                  loss='binary_crossentropy', # Loss function untuk klasifikasi biner
                  metrics=['accuracy'])

cnn_model.summary()

print("\nMemulai pelatihan CNN Model...")

history = cnn_model.fit(
    X_train_pad, Y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_val_pad, Y_val),
    verbose=1
)

print("\n✅ Model CNN berhasil dilatih.")

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

Y_prob_cnn = cnn_model.predict(X_val_pad)

Y_pred_cnn = (Y_prob_cnn > 0.5).astype("int32")

print("\n--- HASIL EVALUASI MODEL CNN (Validation Set) ---")
print("--------------------------------------------------------------------")

print(confusion_matrix(Y_val, Y_pred_cnn))

print("\n--------------------------------------------------------------------")

print("Classification Report:")
print(classification_report(Y_val, Y_pred_cnn, target_names=['Ham (0)', 'Spam (1)']))

cm_cnn = confusion_matrix(Y_val, Y_pred_cnn)

TN, FP, FN, TP = cm_cnn.ravel()

error_data = {
    'Jenis Kesalahan': ['False Positives (FP)', 'False Negatives (FN)'],
    'Jumlah Kejadian': [FP, FN]
}
df_error = pd.DataFrame(error_data)

plt.figure(figsize=(8, 5))
ax = sns.barplot(
    x='Jenis Kesalahan',
    y='Jumlah Kejadian',
    data=df_error,
    palette=['salmon', 'red']
)

for p in ax.patches:
    ax.annotate(f'{p.get_height()}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                fontsize=14,
                color='black',
                xytext=(0, 10),
                textcoords='offset points')

plt.title('Perbandingan Jumlah Kesalahan Kritis Model CNN ')
plt.xlabel('Tipe Kesalahan')
plt.ylabel('Jumlah Pesan Salah Klasifikasi')
plt.ylim(0, max(FP, FN) * 1.2)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print(f"False Positives (FP): {FP}, False Negatives (FN): {FN}")

cm_cnn = confusion_matrix(Y_val, Y_pred_cnn)

TN, FP, FN, TP = cm_cnn.ravel()


class_labels = ['Ham (0)', 'Spam (1)']

plt.figure(figsize=(6, 5))
ax = sns.heatmap(
    cm_cnn,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=class_labels,
    yticklabels=class_labels
)

ax.set_title('Confusion Matrix: CNN Classifier')
ax.set_xlabel('Prediksi Label')
ax.set_ylabel('Aktual Label')

plt.text(
    0.5,
    -0.15,
    f'TN={TN}, FP={FP}\nFN={FN}, TP={TP}',
    horizontalalignment='center',
    verticalalignment='center',
    transform=ax.transAxes,
    fontsize=10,
    color='red'
)

plt.show()

Y_pred_proba_cnn = cnn_model.predict(X_val)

fpr_cnn, tpr_cnn, thresholds_cnn = roc_curve(Y_val, Y_pred_proba_cnn)

roc_auc_cnn = auc(fpr_cnn, tpr_cnn)

plt.figure(figsize=(7, 6))
plt.plot(
    fpr_cnn,
    tpr_cnn,
    color='darkorange',
    lw=2,
    label=f'Kurva ROC CNN (AUC = {roc_auc_cnn:.4f})'
)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Kinerja Acak')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Kurva Receiver Operating Characteristic (ROC) CNN Classifier')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"AUC (Area Under the Curve) CNN: {roc_auc_cnn:.4f}")

Y_pred_proba_cnn = cnn_model.predict(X_val)

df_corr = X_val.copy()
df_corr['predicted_proba'] = Y_pred_proba_cnn

correlation_with_proba = df_corr.corr()['predicted_proba'].drop('predicted_proba')

importance_abs_corr = np.abs(correlation_with_proba)

feature_names = importance_abs_corr.index
df_importance_cnn = pd.DataFrame({
    'feature': feature_names,
    'importance': importance_abs_corr
})

df_importance_cnn = df_importance_cnn.sort_values(
    by='importance',
    ascending=False
).head(10)

print("--- 10 Fitur Paling Penting (Korelasi Absolut dengan Probabilitas Prediksi CNN) ---")
print(df_importance_cnn)

plt.figure(figsize=(10, 6))
sns.barplot(
    x='importance',
    y='feature',
    data=df_importance_cnn,
    color='skyblue'
)
plt.title('Feature Importance (CNN)')
plt.xlabel('Nilai Absolut Korelasi (Tingkat Pengaruh Linier)')
plt.ylabel('Fitur')
plt.show()